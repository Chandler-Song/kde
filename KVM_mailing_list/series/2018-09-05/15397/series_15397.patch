From patchwork Wed Sep  5 14:17:01 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Quan Xu <quan.xu0@gmail.com>
X-Patchwork-Id: 10588955
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 9693C679F
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed,  5 Sep 2018 14:17:21 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 826E02A1B8
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed,  5 Sep 2018 14:17:21 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 767782A22E; Wed,  5 Sep 2018 14:17:21 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-8.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,FREEMAIL_FROM,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1B2772A1B8
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed,  5 Sep 2018 14:17:21 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727626AbeIESrn (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Wed, 5 Sep 2018 14:47:43 -0400
Received: from mail-oi0-f65.google.com ([209.85.218.65]:33685 "EHLO
        mail-oi0-f65.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1727145AbeIESrm (ORCPT <rfc822;kvm@vger.kernel.org>);
        Wed, 5 Sep 2018 14:47:42 -0400
Received: by mail-oi0-f65.google.com with SMTP id 8-v6so13944837oip.0
        for <kvm@vger.kernel.org>; Wed, 05 Sep 2018 07:17:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=to:cc:from:subject:message-id:date:user-agent:mime-version
         :content-transfer-encoding;
        bh=cC1ZlRq7sX/Y97MWuC5DX0coPv69ZSxy+5fmDOGrMXI=;
        b=AvD+IF830ev9RqRYCruOtnJrT7rS97JJN1Qm1qblvVXeNAP5J6qhrtbrFpmEc4wmyz
         UmvZw7qe4vHi5pGqeVZtXpuOaU4QwNbK6RWz453Dh5W3TRmVp2mmVxx1KjrYnrs6RQ2C
         aauLnSgeAuIaov+w8ERMG2Zznnf7VSF7yFSzMwFR/2aOHKOKhcPxqENABrUEN/3vf9Jb
         P4nY67WlVMi5D1l7vwlGc8eSI//zJH/R92UldwOElXMWpxN7aW+MYm2zhV5hXQQNuFXt
         ShdkAdQInNnCK5ENE/fNzlPTbz1uyAdk2+6nkwSq6aq2SMZzPRD4dInb9c55kNPSpVuq
         yuNg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:to:cc:from:subject:message-id:date:user-agent
         :mime-version:content-transfer-encoding;
        bh=cC1ZlRq7sX/Y97MWuC5DX0coPv69ZSxy+5fmDOGrMXI=;
        b=CpypI9yJx489bkByaAbysihD3ZmUjFsnF2X5gnxduz59viiOrRq++J7+/19db85DXf
         tw469H7XPiLSPxCmbxma0nX0kQelyvSqot+XHfSvCNX4z0E4Xw0fR1VqBUXfEmuSvLIL
         lXe07j+MJYhW/JfIJhZT6J9Qi3GVvLl1+Jhf0LHzCKhgHGOKTSl+jsEttITvB3UOXms3
         2dkWoOTfhIyT4HehFnSVHXee7ZERtIB0NbkdqIFsP4bBHoq3bpmIoMToIb6aBjACgPmQ
         6sF1A1S0upQd3YErdEn/YS38mJ1CzrzXcQsPhRxf8StnGFqe3uYhRlUd6I8i6iXQzn6N
         kbJw==
X-Gm-Message-State: APzg51BuyCgMl9nj+xXRMtGd5bxNP2Gk19Q2kK+KZtXLLkhzpkgYxLa+
        pqOkSBQeO08EThFEJk/I2Uj/aCcB
X-Google-Smtp-Source: 
 ANB0VdbbK1td76AZu+LPUzYWB0DxJtoesky4fRgKWw06+zohPICrADYgVF+WtlBgQY/TZorvaJ1aBA==
X-Received: by 2002:aca:a6d4:: with SMTP id
 t81-v6mr29430658oij.113.1536157038978;
        Wed, 05 Sep 2018 07:17:18 -0700 (PDT)
Received: from [192.168.31.103] ([58.35.222.212])
        by smtp.gmail.com with ESMTPSA id
 n84-v6sm1692207oif.23.2018.09.05.07.17.16
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 05 Sep 2018 07:17:18 -0700 (PDT)
To: kvm <kvm@vger.kernel.org>, qemu-devel@nongnu.org
Cc: quintela@redhat.com, "Dr. David Alan Gilbert" <dgilbert@redhat.com>
From: Quan Xu <quan.xu0@gmail.com>
Subject: Subject: [RFC PATCH v2] migration: calculate remaining pages
 accurately during the bulk stage
Message-ID: <c9da5617-ddd8-d122-20c4-79b0bd92c176@gmail.com>
Date: Wed, 5 Sep 2018 22:17:01 +0800
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:60.0) Gecko/20100101
 Thunderbird/60.0
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8; format=flowed
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

From 7de4cc7c944bfccde0ef10992a7ec882fdcf0508 Mon Sep 17 00:00:00 2001
From: Quan Xu <quan.xu0@gmail.com>
Date: Wed, 5 Sep 2018 22:06:58 +0800
Subject: [RFC PATCH v2] migration: calculate remaining pages accurately 
during the bulk stage

Since the bulk stage assumes in (migration_bitmap_find_dirty) that every
page is dirty, initialize the number of dirty pages at the beggining of
the iteration and then decrease it for each processed page.

Signed-off-by: Quan Xu <quan.xu0@gmail.com>
---
  migration/ram.c | 7 ++++++-
  1 file changed, 6 insertions(+), 1 deletion(-)

      }
@@ -2001,6 +2004,7 @@ static bool find_dirty_block(RAMState *rs, 
PageSearchStatus *pss, bool *again)
              /* Flag that we've looped */
              pss->complete_round = true;
              rs->ram_bulk_stage = false;
+            rs->ram_bulk_bytes = 0;
              if (migrate_use_xbzrle()) {
                  /* If xbzrle is on, stop using the data compression at 
this
                   * point. In theory, xbzrle can do better than 
compression.
@@ -2513,6 +2517,7 @@ static void ram_state_reset(RAMState *rs)
      rs->last_page = 0;
      rs->last_version = ram_list.version;
      rs->ram_bulk_stage = true;
+    rs->ram_bulk_bytes = ram_bytes_total();
  }

  #define MAX_WAIT 50 /* ms, half buffered_file limit */
@@ -3308,7 +3313,7 @@ static void ram_save_pending(QEMUFile *f, void 
*opaque, uint64_t max_size,
          /* We can do postcopy, and all the data is postcopiable */
          *res_compatible += remaining_size;
      } else {
-        *res_precopy_only += remaining_size;
+        *res_precopy_only += remaining_size + rs->ram_bulk_bytes;
      }
  }

--
1.8.3.1

diff --git a/migration/ram.c b/migration/ram.c
index 79c8942..1a11436 100644
--- a/migration/ram.c
+++ b/migration/ram.c
@@ -290,6 +290,8 @@ struct RAMState {
      uint32_t last_version;
      /* We are in the first round */
      bool ram_bulk_stage;
+    /* Remaining bytes in the first round */
+    uint64_t ram_bulk_bytes;
      /* How many times we have dirty too many pages */
      int dirty_rate_high_cnt;
      /* these variables are used for bitmap sync */
@@ -1540,6 +1542,7 @@ unsigned long migration_bitmap_find_dirty(RAMState 
*rs, RAMBlock *rb,

      if (rs->ram_bulk_stage && start > 0) {
          next = start + 1;
+        rs->ram_bulk_bytes -= TARGET_PAGE_SIZE;
      } else {
          next = find_next_bit(bitmap, size, start);
